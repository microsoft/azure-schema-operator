[{"id":0,"href":"/azure-schema-operator/contributing/contributing/","title":"Contributing","section":"For Contributors","content":"Contributing to Azure Schema Operator #  Developer setup (with VS Code) #  This is the recommended setup, especially if you are using Windows as your development platform.\nThis repository contains a devcontainer configuration that can be used in conjunction with VS Code to set up an environment with all the required tools preinstalled.\nIf you want to use this:\n Make sure you have installed the prerequisites to use Docker, including WSL if on Windows.\n  Install VS Code and the Remote Development extension (check installation instructions there).\n  Run the VS Code command (with Ctrl-Shift-P): Remote Containers: Clone Repository in Container Volume...\nNote: in Windows, it is important to clone directly into a container instead of cloning first and then loading that with the Remote Containers extension, as the tooling performs a lot of file I/O, and if this is performed against a volume mounted in WSL then it is unusably slow.\nTo complete the clone:\n Select \u0026ldquo;GitHub\u0026rdquo;. Search for \u0026ldquo;Microsoft/azure-schema-operator\u0026rdquo;. Choose either of the following options about where to create the volume. The window will reload and run the Dockerfile setup. The first time, this will take some minutes to complete as it installs all dependencies. Run git submodule init and git submodule update    To validate everything is working correctly, you can open a terminal in VS Code and run task -l. This will show a list of all task commands. Running task by itself (or task default) will run quick local pre-checkin tests and validation.\n  Without VS Code #  Option 1: Dockerfile #  The same Dockerfile that the VS Code devcontainer extension uses can also be used outside of VS Code; it is stored in the root .devcontainer directory and can be used to create a development container with all the tooling preinstalled:\n$ docker build $(git rev-parse --show-toplevel)/.devcontainer -t asodev:latest … image will be created … $ # After that you can start a terminal in the development container with: $ docker run --env-file ~/work/envs.env -v $(git rev-parse --show-toplevel):/go/src -w /go/src -u $(id -u ${USER}):$(id -g ${USER}) --group-add $(stat -c \u0026#39;%g\u0026#39; /var/run/docker.sock) -v /var/run/docker.sock:/var/run/docker.sock --network=host -it asodev:latest /bin/bash It is not recommended to mount the source like this on Windows (WSL2) as the cross-VM file operations are very slow.\nOption 2: ./dev.sh #  If you are using Linux, instead of using VS Code you can run the dev.sh script in the root of the repository. This will install all required tooling into the hack/tools directory and then start a new shell with the PATH updated to use it.\nDirectory structure of the operator #  Running integration tests #  Basic use: run task controller:test-integration-envtest.\nThe task controller:test-integration-envtest runs the tests on mocks by default, so that it does not touch any live Azure database.\nRunning live tests #  If you want to run tests against live Azure databases, you can use the controller:test-integration-envtest-live task. This will run tests with LIVE_TEST=true environemnt variable which add tests versos live Azure databases. This will also require you to set the authentication environment variables, as detailed in the authentication document.\nRunning a single test #  By default task controller:test-integration-envtest and its variants run all tests. This is often undesirable as you may just be working on a single feature or test. In order to run a subset of tests, use:\nTEST_FILTER=test_name_regex task controller:test-integration-envtest Running the operator locally #  If you would like to try something out but do not want to write an integration test, you can run the operation locally in a kind cluster.\nBefore launching kind, make sure that your shell has the AZURE_SUBSCRIPTION_ID, AZURE_TENANT_ID, AZURE_CLIENT_ID, and AZURE_CLIENT_SECRET environment variables set. See above for more details about them.\nOnce you\u0026rsquo;ve set the environment variables above, run one of the following commands to create a kind cluster:\n Service Principal authentication cluster: task controller:kind-create-with-service-principal. AAD Pod Identity authentication enabled cluster (emulates Managed Identity): controller:kind-create-with-podidentity.  You can use kubectl to interact with the local kind cluster.\nWhen you\u0026rsquo;re done with the local cluster, tear it down with task controller:kind-delete.\nSubmitting a pull request #  Pull requests opened from forks of the azure-schema-operator repository will initially have a skipped Validate Pull Request / integration-tests check which will prevent merging even if all other checks pass. Once a maintainer has looked at your PR and determined it is ready they will comment /ok-to-test sha=\u0026lt;sha\u0026gt; to kick off an integration test pass. If this check passes along with the other checks the PR can be merged.\nCommon problems and their solutions #  "},{"id":1,"href":"/azure-schema-operator/introduction/helm-docs/","title":"Helm Docs","section":"User’s Guide","content":"azure-schema-operator #  Deploy components and dependencies of azure-schema-operator\nHomepage: https://github.com/Microsoft/azure-schema-operator\nMaintainers #     Name Email Url     Jony Vesterman Cohen jony.cohen@microsoft.com     Source Code #   https://github.com/Microsoft/azure-schema-operator  Values #     Key Type Default Description     ServiceMonitor bool false    autoscaling.enabled bool false    autoscaling.maxReplicas int 100    autoscaling.minReplicas int 1    autoscaling.targetCPUUtilizationPercentage int 80    azureClientID string \u0026quot;\u0026quot;    azureClientSecret string \u0026quot;\u0026quot;    azureIdentitySelector string \u0026quot;azureschemaoperator-manager-binding\u0026quot;    azureResourceId string \u0026quot;\u0026quot;    azureTenantID string \u0026quot;\u0026quot;    createAzureOperatorSecret bool false    createAzurePodIdentity bool false    image.pullPolicy string \u0026quot;IfNotPresent\u0026quot;    image.repository string \u0026quot;ghcr.io/microsoft/azure-schema-operator/azureschemaoperator:v1.0.1\u0026quot;    image.tag string \u0026quot;\u0026quot;    replicaCount int 1    resources object {}    serviceAccount.annotations object {}    serviceAccount.create bool true    serviceAccount.name string \u0026quot;\u0026quot;      Autogenerated from chart metadata using helm-docs v1.10.0\n"},{"id":2,"href":"/azure-schema-operator/contributing/create-a-new-release/","title":"Create a New Release","section":"For Contributors","content":"Creating a new release of the Azure-Schema-Operator #   Go to the releases page and draft a new release. In the tag dropdown, type the name of the new tag you\u0026rsquo;d like to create (it should match the pattern of previous releases tags, for example: v1.0.0-alpha.1). The release target should be main (the default). Use the GitHub \u0026ldquo;auto-generate release notes\u0026rdquo; button to generate a set of release notes to work with. You will need to clean this up quite a bit before actually publishing it. If publishing an alpha/beta, be sure to mark the release as a pre-release. Write a \u0026ldquo;Release Notes\u0026rdquo; section. You can edit the autogenerated section as a start. You can also look through the commits between the last release and now: git log v1.0.0-alpha.6..main. Publish the release. This will automatically trigger a GitHub action to build and publish an updated Docker image with the latest manager changes. Ensure that the action associated with your release finishes successfully.  Creating and testing Helm chart for new release #    Create a new branch from \u0026lt;NEW_RELEASE_TAG\u0026gt; HEAD\n  Generate helm manifest for new release: task controller:gen-helm-manifest\n  Check the version in /charts/azure-schema-operator/Chart.yaml if matches with the latest release tag.\n  Install helm chart:\nhelm install schemaop -n azureschemaoperator-system --create-namespace ./charts/azure-schema-operator/.   Wait for the chart installation.\n  Wait for it to start: kubectl get all -n azureschemaoperator-system\n  Run through the kusto tutorial and validate the changes.\n  If installed successfully, commit the files under charts/azure-schema-operator.\n  Create a pool request following the PR template.\n  Fixing an incorrect release #  If there was an issue publishing a new release, we may want to delete the existing release and try again. Only do this if you\u0026rsquo;ve just published the release and there is something wrong with it. We shouldn\u0026rsquo;t be deleting releases people are actually using.\n Delete the release in the releases page. Delete the tag: git push \u0026lt;origin\u0026gt; --delete \u0026lt;tag\u0026gt;, for example git push origin --delete v2.0.0-alpha.1.  At this point, you can safely publish a \u0026ldquo;new\u0026rdquo; release with the same name.\n"},{"id":3,"href":"/azure-schema-operator/introduction/annotations/","title":"Annotations","section":"User’s Guide","content":"Annotations understood by the operator #  Annotations specified by the user #  Note that unless otherwise specified, allowed values are case sensitive and should be provided in lower case.\nserviceoperator.azure.com/reconcile-policy #  Specifies the reconcile policy to use. Allowed values are:\n manage: The operator performs all actions as normal. This is the default if no annotation is specified. skip: All modification actions on the backing Azure resource are skipped. GETs are still issued to ensure that the resource exists. If the resource doesn\u0026rsquo;t exist, the Ready condition will show a Warning state with the details of the missing resource until the resource is created. If the resource is deleted in Kubernetes, it is not deleted in Azure. In REST API terminology, PUT and DELETE are skipped while GET is allowed. detach-on-delete: Modifications are pushed to the backing Azure resource, but if the resource is deleted in Kubernetes, it is not deleted in Azure. In REST API terminology, PUT and GET are allowed while DELETE is skipped.  Unknown values default to manage.\nAnnotations written by the operator #  These annotations are written by the operator for its own internal use. Their existence and usage may change in the future. We recommend users avoid depending upon these annotations:\n serviceoperator.azure.com/resource-id: The ARM resource ID. serviceoperator.azure.com/poller-resume-token: JSON encoded token for polling long running operation. serviceoperator.azure.com/poller-resume-id: ID describing the poller to use.  "},{"id":4,"href":"/azure-schema-operator/introduction/authentication/","title":"Authentication","section":"User’s Guide","content":"Authentication in Azure Schema Operator #  Azure Schema Operator supports two different styles of authentication today.\n managed identity (via aad-pod-identity authentication) Service Principal  Managed Identity (aad-pod-identity) #  Prerequisites #   An existing Azure Managed Identity. aad-pod-identity installed into your cluster. If you are running ASO on an Azure Kubernetes Service (AKS) cluster, you can instead use the integrated aad-pod-identity.  First, set the following environment variables:\nexport IDENTITY_RESOURCE_GROUP=\u0026#34;myrg\u0026#34; # The resource group containing the managed identity. export IDENTITY_NAME=\u0026#34;myidentity\u0026#34; # The name of the identity. export AZURE_SUBSCRIPTION_ID=\u0026#34;00000000-0000-0000-0000-00000000000\u0026#34; # The Azure Subscription ID the identity is in. export AZURE_TENANT_ID=\u0026#34;00000000-0000-0000-0000-00000000000\u0026#34; # The Azure AAD Tenant the identity/subscription is associated with. Use the az cli to get some more details about the identity to use:\nexport IDENTITY_CLIENT_ID=\u0026#34;$(az identity show -g ${IDENTITY_RESOURCE_GROUP} -n ${IDENTITY_NAME} --query clientId -otsv)\u0026#34; export IDENTITY_RESOURCE_ID=\u0026#34;$(az identity show -g ${IDENTITY_RESOURCE_GROUP} -n ${IDENTITY_NAME} --query id -otsv)\u0026#34; Deploy an AzureIdentity:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: \u0026#34;aadpodidentity.k8s.io/v1\u0026#34; kind: AzureIdentity metadata: name: azureschemaoperator-identity namespace: azureschemaoperator-system spec: type: 0 resourceID: ${IDENTITY_RESOURCE_ID} clientID: ${IDENTITY_CLIENT_ID} EOF Deploy an AzureIdentityBinding to bind this identity to the Azure Schema Operator manager pod:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: \u0026#34;aadpodidentity.k8s.io/v1\u0026#34; kind: AzureIdentityBinding metadata: name: azureschemaoperator-identity-binding namespace: azureschemaoperator-system spec: azureIdentity: azureschemaoperator-identity selector: azureschemaoperator-manager-binding EOF Service Principal #  Prerequisites #   An existing Azure Service Principal.  To use Service Principal authentication, specify an schema-operator-controller-settings secret with AZURE_CLIENT_ID and AZURE_CLIENT_SECRET set.\n AZURE_CLIENT_ID must be set to the Service Principal client ID. This will be a GUID. AZURE_CLIENT_SECRET must be set to the Service Principal client secret.  For more information about Service Principals, see creating an Azure Service Principal using the Azure CLI. The AZURE_CLIENT_ID is sometimes also called the App ID. The AZURE_CLIENT_SECRET is the \u0026ldquo;password\u0026rdquo; returned by the command in the previously linked documentation.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: schema-operator-controller-settings namespace: azureschemaoperator-system stringData: AZURE_TENANT_ID: \u0026#34;$AZURE_TENANT_ID\u0026#34; AZURE_CLIENT_ID: \u0026#34;$AZURE_CLIENT_ID\u0026#34; AZURE_CLIENT_SECRET: \u0026#34;$AZURE_CLIENT_SECRET\u0026#34; EOF Note - When deploying the schema operator via the helm chart the secret can be generated by passing createAzureOperatorSecret: true to the Values.yaml.\n"},{"id":5,"href":"/azure-schema-operator/introduction/configuration/","title":"Configuration","section":"User’s Guide","content":"Configuration #  Schema ConfigMap #  SQL Server #  The SQL SERVER configmap supports a few extra options:\n sqlpackageOptions - a string of options (space seperated) to pass to the sqlpackage executable.  "},{"id":6,"href":"/azure-schema-operator/introduction/events_and_conditions/","title":"Events and Conditions","section":"User’s Guide","content":"Events and Conditions #  Azure Schema Operator exposes varios events and conditions to allow for simpler status observation.\nConditions #  We can wait for a SchemaDeployment to finish by waiting on the Execution condition:\n➜ kubectl wait --for=condition=Execution --timeout=10s schemadeployment/master-test-template schemadeployment.dbschema.microsoft.com/master-test-template condition met The condition is also displayed when we get the SchemaDeployment object:\n➜ kubectl get schemadeployments master-test-template NAME TYPE EXECUTED master-test-template kusto True In case of failuer the Execution condition will be marked as such:\n➜ kubectl get schemadeployments master-test-template NAME TYPE EXECUTED master-test-template kusto False Events #  Dureing the deployment process events will be reported on the different steps and changes that occur.\nEvents differ according to the type and required deployment, and can easily be seen from kubectl:\n1 2 3 4  ➜ kubectl get events --field-selector involvedObject.name=master-test-template LAST SEEN TYPE REASON OBJECT MESSAGE 6m36s Normal Created schemadeployment/master-test-template Created versioned deployment \u0026#34;master-test-template-0\u0026#34; 5m36s Normal Executed schemadeployment/master-test-template Scheme was deployed   "},{"id":7,"href":"/azure-schema-operator/introduction/filter/","title":"Filter","section":"User’s Guide","content":"Target Filter #  The operator uses the TargetFilter struct to Filter and aquire execution targets. The semantics change a bit between the different db technologies to match common use patterns.\nAlthough the general notions stay the same.\nClusterUris holds a list of clusters/servers/Eventhub namespaces. Create flag indicates if we should create the DB/schema/registry if missing. Regexp flag indicates if we should regard the filter values as regular expressions or exact match.\nKusto filtering #  In Kusto a common multi-tenantcy solution is DB per tenant, To support this scenario we use the DB field as a regular expression to filter all the database names in the clusters.\nSometimes an external system is used to determain the schema type instead of DB name, e.g. if we have different tier users. To support this scenario we have a Webhook \u0026amp; Label system, we will make a rest call to that webhook and passing the label. The response is expected to be a json array with database names on which we should apply the schema.\nSQL Server filtering #  In Sql Server a common multi-tenantcy solution is Schema per tenant, to support this scenario we use the Schema field as a regular expression to filter all the schema names. Note In this scenario we regard the DB name as exact match.\nEventhub schema registry #  In Eventhubs we can only define one schema registry - we match the name according to the DB field.\n"},{"id":8,"href":"/azure-schema-operator/introduction/install/","title":"Install","section":"User’s Guide","content":"Installation Guide #  This document will guide you through the installation and configuration process. The Schema-Operator needs a client account that can access the Azure Data Explorer clusters you wish to manage and as this is an Operator we need an AKS cluster.\nPrerequisits #   An Azure Subscription to provision resources into. An Azure Service Principal for the operator to use. see Authentication docs for further details. A Kubernetes Cluster created and running.  Identity #  Schema operator uses a managed identity (MSI) to access the managed clusters. Please create a managed identity and assign administrative permissions for the operator to change the databases and schemas.\nInstallation #  Schema-Operator is deployed using a helm chart. In the provided values we need to pass the MSI name to bind.\nexport VERSION=1.0.1 chart=https://github.com/microsoft/azure-schema-operator/releases/download/v${VERSION}/azure-schema-operator-v${VERSION}.tgz chart=charts/azure-schema-operator-v${VERSION}.tgz helm install schema-operator-test $chart --namespace=schema-operator-test --create-namespace more details on chart parameters can be found at the chart docs\nDeployment in Dev environment #  When developing it\u0026rsquo;s possible to deploy from the repo using make deploy\nIt will use the aks configured in the local system to deploy the crds and deployment from the config/ folder.\nexport ACR=\u0026lt;your acr\u0026gt; export VERSION=0.0.4 export OPERATOR_IMG=\u0026#34;${ACR}.azurecr.io/schema-operator:v${VERSION}\u0026#34; az acr login -n ${ACR} make docker-build-push IMG=$OPERATOR_IMG make deploy IMG=$OPERATOR_IMG "},{"id":9,"href":"/azure-schema-operator/introduction/plugin_installation/","title":"Plugin Installation","section":"User’s Guide","content":"Plugin Installation Guide #  TBD\n"},{"id":10,"href":"/azure-schema-operator/tutorials/eventhub/","title":"Eventhub","section":"Tutorials","content":"Eventhub schema regitery tutorial #  Creating the ConfigMap:\nkubectl create configmap event-demo --from-literal templateName=\u0026#34;schemaop\u0026#34; --from-literal group=\u0026#34;testsgr\u0026#34; \\ --from-file=schema=docs/samples/eventhubs/avro-schema.json next we need to define a SchemaDeployment object that will reference the ConfigMap.\napiVersion: dbschema.microsoft.com/v1alpha1 kind: SchemaDeployment metadata: name: eventhub-schema-demo spec: type: eventhub applyTo: clusterUris: [\u0026#39;schematest.servicebus.windows.net\u0026#39;] failIfDataLoss: false failurePolicy: abort source: name: event-demo namespace: default and apply it via kubectl:\nkubectl apply -f docs/samples/eventhubs/eventhub-schema-demo.yaml To demonstrate schema evolution, we will add a new field:\n{ \u0026#34;name\u0026#34;: \u0026#34;description\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } let\u0026rsquo;s run:\nkubectl create configmap event-demo --from-literal templateName=\u0026#34;schemaop\u0026#34; --from-literal group=\u0026#34;testsgr\u0026#34; \\ --from-file=schema=docs/samples/eventhubs/avro-schema-v2.json --dry-run=client -o yaml | kubectl apply -f - To see the deplopyment status we can check the history:\nkubectl schemaop history --namespace default --name eventhub-schema-demo NAMESPACE NAME REVISION default eventhub-schema-demo-0 0 default eventhub-schema-demo-1 1 "},{"id":11,"href":"/azure-schema-operator/tutorials/kusto/","title":"Kusto","section":"Tutorials","content":"Azure Data Explorer (ADX, AKA Kusto) Tutorial #  In this short tutorial we will review the process of managing a schema, performing a change and rollback in the case of an error. We will deploy 3 revisions of our schema, with an error on the third schema triggering a rollback.\nThe schema is represented in kql field in a standard ConfigMap which contains ADX schemas described as KQLs.. More details on KQL files can be found in the delta-kusto instructions\nOr simply download and review a sample kql file\nOnce we have a kql describe our schema we can generate an example ConfigMap using:\nkubectl create configmap test-sample-kql --from-file=kql=sample.kql --dry-run=client -o yaml We reference the ConfigMap object from the SchemaDeployment object to apply onto the clusters.\nAll The objects used throughout the tutorial can be found in samples folder\nPre-requisits #  the tutorial assumes that the Schema operator is already installed with the appropriate permissions - if not, please see installation While not mandetory, a kubectl plugin exists that provides simpler access to the schema revision history - see plugin installation for details\nTutorial steps #  The first step is to create our first schema ConfigMap, later to be deployed to our cluster:\nkubectl apply -f docs/samples/kusto/sample-cm.yml With the schema\u0026rsquo;s ConfigMap in place we are ready to deploy to our test cluster:\nkubectl apply -f docs/samples/kusto/sample-sd.yml Once we\u0026rsquo;ve created the necessary k8s objects, we should check the schema deployment execution status by getting the SchemaDeployment object:\n➜ kubectl get schemadeployments sample-adx NAME TYPE EXECUTED sample-adx kusto True To update the schema, we should simply patch the ConfigMap, which will trigger the schema operator to validate the schema, and apply updates if needed.\nkubectl apply -f docs/samples/kusto/sample-cm-v2.yml To view current status and history we can use the provided plugin:\nkubectl schemaop status --namespace default --name sample-adx NAMESPACE NAME REVISION EXECUTED FAILED RUNNING SUCCEEDED default sample-adx-1 1 true 0 0 1 kubectl schemaop history --namespace default --name sample-adx NAMESPACE NAME REVISION default sample-adx-0 0 default sample-adx-1 1 Now, lets make things a bit more interesting, and apply a schema that contains an error:\nkubectl apply -f docs/samples/kusto/sample-cm-err.yml If we check the status we will see we are now on our 4th revision!\nkubectl schemaop history --namespace default --name sample-adx NAMESPACE NAME REVISION default sample-adx-0 0 default sample-adx-1 1 default sample-adx-2 2 default sample-adx-3 3 Further checking the history we can see that revision 2 (the 3rd revision) failed:\nkubectl schemaop history --namespace default --name sample-adx --revision 2 NAMESPACE NAME REVISION EXECUTED FAILED RUNNING SUCCEEDED default sample-adx-2 2 false 1 1 0 We can also see, that the 4th and 2nd revisions are the same. That\u0026rsquo;s because we rolled back the faulty schema revision as requested by the template object:\nspec: failurePolicy: rollback Summary #  We\u0026rsquo;ve looked at a common flow and how Schema-Operator can ease managment of large schema deployments. We suggest trying some of these scenarios in a dev environment to get familiar with the different options.\n"},{"id":12,"href":"/azure-schema-operator/tutorials/sqlserver/","title":"Sqlserver","section":"Tutorials","content":"SQL Server Tutorial #  A common multi-tenancy architecture for sqlserver is using a schema per tenant. This tutorial will show how to deploy the schema given in a DACPAC format to each schema in the database.\nThe tutorial uses a sample dacpac with a sales schema.\nMSI is used to authenticate in the tutorial as it\u0026rsquo;s simpler.\nwe need to add the MSI as a user to the DB, e.g.:\nCREATE USER [schema-operator-msi] FROM EXTERNAL PROVIDER; GO ALTER ROLE db_datareader ADD MEMBER [schema-operator-msi]; ALTER ROLE db_datawriter ADD MEMBER [schema-operator-msi]; ALTER ROLE db_owner ADD MEMBER [schema-operator-msi]; GRANT EXECUTE TO [schema-operator-msi] GO Creating the ConfigMap:\nkubectl create configmap dacpac-config --from-literal templateName=\u0026#34;SalesLT\u0026#34; --from-file=dacpac=docs/samples/sqlserver/test.dacpac next we need to define a SchemaDeployment object that will reference the ConfigMap.\napiVersion: dbschema.microsoft.com/v1alpha1 kind: SchemaDeployment metadata: name: sql-demo-deployment spec: type: sqlServer applyTo: clusterUris: [\u0026#39;schematest.database.windows.net\u0026#39;] db: \u0026#39;db1\u0026#39; schema: test failIfDataLoss: true failurePolicy: abort source: name: dacpac-config namespace: default and apply it via kubectl:\nkubectl apply -f docs/samples/sqlserver/sql-demo-deployment.yaml External Dacpacs #  For cases where the project has external dacpac references we can add them as a reference from the schema ConfigMap like this:\nkubectl create configmap common-config --from-file=dacpac=./DBOCommon.dacpac kubectl create configmap tenant-config --from-literal templateName=\u0026#34;MasterSchema\u0026#34; \\ --from-literal externalDacpacs=\u0026#39;{ \u0026#34;DBOCommon\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;common-config\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;}}\u0026#39; \\ --from-file=dacpac=tenant.dacpac The external Dacpac requires a seperate external SchemaDeployment object to deploy it ( to fully capsulate the \u0026ldquo;externallism\u0026rdquo; of it)\nNote as the name of the external DacPac matters we need to pass this name - so it is the \u0026ldquo;key\u0026rdquo; for the reference.\n"}]