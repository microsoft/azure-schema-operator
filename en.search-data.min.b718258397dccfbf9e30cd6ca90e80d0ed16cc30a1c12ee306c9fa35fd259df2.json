[{"id":0,"href":"/azure-schema-operator/contributing/contributing/","title":"Contributing","section":"For Contributors","content":"Contributing to Azure Schema Operator #  Developer setup (with VS Code) #  This is the recommended setup, especially if you are using Windows as your development platform.\nThis repository contains a devcontainer configuration that can be used in conjunction with VS Code to set up an environment with all the required tools preinstalled.\nIf you want to use this:\n Make sure you have installed the prerequisites to use Docker, including WSL if on Windows.\n  Install VS Code and the Remote Development extension (check installation instructions there).\n  Run the VS Code command (with Ctrl-Shift-P): Remote Containers: Clone Repository in Container Volume...\nNote: in Windows, it is important to clone directly into a container instead of cloning first and then loading that with the Remote Containers extension, as the tooling performs a lot of file I/O, and if this is performed against a volume mounted in WSL then it is unusably slow.\nTo complete the clone:\n Select \u0026ldquo;GitHub\u0026rdquo;. Search for \u0026ldquo;Microsoft/azure-schema-operator\u0026rdquo;. Choose either of the following options about where to create the volume. The window will reload and run the Dockerfile setup. The first time, this will take some minutes to complete as it installs all dependencies. Run git submodule init and git submodule update    To validate everything is working correctly, you can open a terminal in VS Code and run task -l. This will show a list of all task commands. Running task by itself (or task default) will run quick local pre-checkin tests and validation.\n  Without VS Code #  Option 1: Dockerfile #  The same Dockerfile that the VS Code devcontainer extension uses can also be used outside of VS Code; it is stored in the root .devcontainer directory and can be used to create a development container with all the tooling preinstalled:\n$ docker build $(git rev-parse --show-toplevel)/.devcontainer -t asodev:latest … image will be created … $ # After that you can start a terminal in the development container with: $ docker run --env-file ~/work/envs.env -v $(git rev-parse --show-toplevel):/go/src -w /go/src -u $(id -u ${USER}):$(id -g ${USER}) --group-add $(stat -c \u0026#39;%g\u0026#39; /var/run/docker.sock) -v /var/run/docker.sock:/var/run/docker.sock --network=host -it asodev:latest /bin/bash It is not recommended to mount the source like this on Windows (WSL2) as the cross-VM file operations are very slow.\nOption 2: ./dev.sh #  If you are using Linux, instead of using VS Code you can run the dev.sh script in the root of the repository. This will install all required tooling into the hack/tools directory and then start a new shell with the PATH updated to use it.\nDirectory structure of the operator #  Running integration tests #  Basic use: run task controller:test-integration-envtest.\nRecord/replay #  The task controller:test-integration-envtest runs the tests in a record/replay mode by default, so that it does not touch any live Azure resources. (This uses the go-vcr library.) If you change the controller or other code in such a way that the required requests/responses from ARM change, you will need to update the recordings.\nTo do this, delete the recordings for the failing tests (under {test-dir}/recordings/{test-name}.yml), and re-run controller:test-integration-envtest. If the test passes, a new recording will be saved, which you can commit to include with your change. All authentication and subscription information is removed from the recording.\nTo run the test and produce a new recording you will also need to have set the required authentication environment variables for an Azure Service Principal: AZURE_SUBSCRIPTION_ID, AZURE_TENANT_ID, AZURE_CLIENT_ID, and AZURE_CLIENT_SECRET. This Service Principal will need access to the subscription to create and delete resources.\nIf you need to create a new Azure Service Principal, run the following commands:\n$ az login … follow the instructions … $ az account set --subscription {the subscription ID you would like to use} Creating a role assignment under the scope of \u0026#34;/subscriptions/{subscription ID you chose}\u0026#34; … $ az ad sp create-for-rbac --role contributor --name {the name you would like to use} { \u0026#34;appId\u0026#34;: \u0026#34;…\u0026#34;, \u0026#34;displayName\u0026#34;: \u0026#34;{name you chose}\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;{name you chose}\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;…\u0026#34;, \u0026#34;tenant\u0026#34;: \u0026#34;…\u0026#34; } The output contains appId (AZURE_CLIENT_ID), password (AZURE_CLIENT_SECRET), and tenant (AZURE_TENANT_ID). Store these somewhere safe as the password cannot be viewed again, only reset. The Service Principal will be created as a “contributor” to your subscription which means it can create and delete resources, so ensure you keep the secrets secure.\nRunning live tests #  If you want to skip all recordings and run all tests directly against live Azure resources, you can use the controller:test-integration-envtest-live task. This will also require you to set the authentication environment variables, as detailed above.\nRunning a single test #  By default task controller:test-integration-envtest and its variants run all tests. This is often undesirable as you may just be working on a single feature or test. In order to run a subset of tests, use:\nTEST_FILTER=test_name_regex task controller:test-integration-envtest Running the operator locally #  If you would like to try something out but do not want to write an integration test, you can run the operation locally in a kind cluster.\nBefore launching kind, make sure that your shell has the AZURE_SUBSCRIPTION_ID, AZURE_TENANT_ID, AZURE_CLIENT_ID, and AZURE_CLIENT_SECRET environment variables set. See above for more details about them.\nOnce you\u0026rsquo;ve set the environment variables above, run one of the following commands to create a kind cluster:\n Service Principal authentication cluster: task controller:kind-create-with-service-principal. AAD Pod Identity authentication enabled cluster (emulates Managed Identity): controller:kind-create-with-podidentity.  You can use kubectl to interact with the local kind cluster.\nWhen you\u0026rsquo;re done with the local cluster, tear it down with task controller:kind-delete.\nSubmitting a pull request #  Pull requests opened from forks of the azure-service-operator repository will initially have a skipped Validate Pull Request / integration-tests check which will prevent merging even if all other checks pass. Once a maintainer has looked at your PR and determined it is ready they will comment /ok-to-test sha=\u0026lt;sha\u0026gt; to kick off an integration test pass. If this check passes along with the other checks the PR can be merged.\nCommon problems and their solutions #  "},{"id":1,"href":"/azure-schema-operator/introduction/helm-docs/","title":"Helm Docs","section":"User’s Guide","content":"azure-schema-operator #  Deploy components and dependencies of azure-schema-operator\nHomepage: https://github.com/Microsoft/azure-schema-operator\nMaintainers #     Name Email Url     Jony Vesterman Cohen jony.cohen@microsoft.com     Source Code #   https://github.com/Microsoft/azure-schema-operator  Values #     Key Type Default Description     ServiceMonitor bool false    autoscaling.enabled bool false    autoscaling.maxReplicas int 100    autoscaling.minReplicas int 1    autoscaling.targetCPUUtilizationPercentage int 80    azureClientID string \u0026quot;\u0026quot;    azureClientSecret string \u0026quot;\u0026quot;    azureIdentitySelector string \u0026quot;azureschemaoperator-manager-binding\u0026quot;    azureResourceId string \u0026quot;\u0026quot;    azureTenantID string \u0026quot;\u0026quot;    createAzureOperatorSecret bool false    createAzurePodIdentity bool false    image.pullPolicy string \u0026quot;IfNotPresent\u0026quot;    image.repository string \u0026quot;ghcr.io/microsoft/azure-schema-operator/azureschemaoperator:v1.0.1\u0026quot;    image.tag string \u0026quot;\u0026quot;    replicaCount int 1    resources object {}    serviceAccount.annotations object {}    serviceAccount.create bool true    serviceAccount.name string \u0026quot;\u0026quot;      Autogenerated from chart metadata using helm-docs v1.10.0\n"},{"id":2,"href":"/azure-schema-operator/contributing/create-a-new-release/","title":"Create a New Release","section":"For Contributors","content":"Creating a new release of ASO v2 #   Go to the releases page and draft a new release. In the tag dropdown, type the name of the new tag you\u0026rsquo;d like to create (it should match the pattern of previous releases tags, for example: v1.0.0-alpha.1). The release target should be main (the default). Use the GitHub \u0026ldquo;auto-generate release notes\u0026rdquo; button to generate a set of release notes to work with. You will need to clean this up quite a bit before actually publishing it. If publishing an alpha/beta, be sure to mark the release as a pre-release. Write a \u0026ldquo;Release Notes\u0026rdquo; section. You can edit the autogenerated section as a start. You can also look through the commits between the last release and now: git log v1.0.0-alpha.6..main. Publish the release. This will automatically trigger a GitHub action to build and publish an updated Docker image with the latest manager changes. Ensure that the action associated with your release finishes successfully.  Testing the new release #    Download the yaml file from the release page\n  Create a kind cluster: task controller:kind-create\n  Create the namespace for the operator: k create namespace azureserviceoperator-system\n  Source the SP credentials to use for the secret and then run ./scripts/deploy_testing_secret.sh sp\n  Deploy the operator from MCR: k apply --server-side=true -f \u0026lt;path-to-downloaded-yaml\u0026gt; (We need to use server-side apply because the CRD for VirtualMachines is large enough that it can\u0026rsquo;t fit in the last-applied-configuration annotation client-side kubectl apply uses.)\n  Wait for it to start: k get all -n azureserviceoperator-system\n  Create a resource group and a vnet in it (the vnet is to check that conversion webhooks are working, since there aren\u0026rsquo;t any for RGs):\nk apply -f v2/config/samples/resources/v1alpha1api20200601_resourcegroup.yaml k apply -f v2/config/samples/network/v1alpha1api20201101_virtualnetwork.yaml   Make sure they deploy successfully - check in the portal as well.\n  Creating and testing Helm chart for new release #    Create a new branch from \u0026lt;NEW_RELEASE_TAG\u0026gt; HEAD\n  Generate helm manifest for new release: task controller:gen-helm-manifest\n  Check the version in /charts/azure-schema-operator/Chart.yaml if matches with the latest release tag.\n  Install helm chart:\nhelm install --set azureSubscriptionID=$AZURE_SUBSCRIPTION_ID \\ --set azureTenantID=$AZURE_TENANT_ID \\ --set azureClientSecret=$AZURE_CLIENT_SECRET \\ --set azureClientID=$AZURE_CLIENT_ID \\ schemaop -n azureschemaoperator-system --create-namespace ./charts/azure-schema-operator/.   Wait for the chart installation.\n  Wait for it to start: k get all -n azureserviceoperator-system\n  Create a resource group and a vnet in it (the vnet is to check that conversion webhooks are working, since there aren\u0026rsquo;t any for RGs):\nk apply -f v2/config/samples/resources/v1alpha1api20200601_resourcegroup.yaml k apply -f v2/config/samples/network/v1alpha1api20201101_virtualnetwork.yaml   Make sure they deploy successfully - check in the portal as well.\n  If installed successfully, commit the files under v2/charts/azure-service-operator.\n  Send a PR.\n  Fixing an incorrect release #  If there was an issue publishing a new release, we may want to delete the existing release and try again. Only do this if you\u0026rsquo;ve just published the release and there is something wrong with it. We shouldn\u0026rsquo;t be deleting releases people are actually using.\n Delete the release in the releases page. Delete the tag: git push \u0026lt;origin\u0026gt; --delete \u0026lt;tag\u0026gt;, for example git push origin --delete v2.0.0-alpha.1.  At this point, you can safely publish a \u0026ldquo;new\u0026rdquo; release with the same name.\n"},{"id":3,"href":"/azure-schema-operator/introduction/annotations/","title":"Annotations","section":"User’s Guide","content":"Annotations understood by the operator #  Annotations specified by the user #  Note that unless otherwise specified, allowed values are case sensitive and should be provided in lower case.\nserviceoperator.azure.com/reconcile-policy #  Specifies the reconcile policy to use. Allowed values are:\n manage: The operator performs all actions as normal. This is the default if no annotation is specified. skip: All modification actions on the backing Azure resource are skipped. GETs are still issued to ensure that the resource exists. If the resource doesn\u0026rsquo;t exist, the Ready condition will show a Warning state with the details of the missing resource until the resource is created. If the resource is deleted in Kubernetes, it is not deleted in Azure. In REST API terminology, PUT and DELETE are skipped while GET is allowed. detach-on-delete: Modifications are pushed to the backing Azure resource, but if the resource is deleted in Kubernetes, it is not deleted in Azure. In REST API terminology, PUT and GET are allowed while DELETE is skipped.  Unknown values default to manage.\nAnnotations written by the operator #  These annotations are written by the operator for its own internal use. Their existence and usage may change in the future. We recommend users avoid depending upon these annotations:\n serviceoperator.azure.com/resource-id: The ARM resource ID. serviceoperator.azure.com/poller-resume-token: JSON encoded token for polling long running operation. serviceoperator.azure.com/poller-resume-id: ID describing the poller to use.  "},{"id":4,"href":"/azure-schema-operator/introduction/authentication/","title":"Authentication","section":"User’s Guide","content":"Authentication in Azure Schema Operator #  Azure Schema Operator supports two different styles of authentication today.\n managed identity (via aad-pod-identity authentication) Service Principal  Managed Identity (aad-pod-identity) #  Prerequisites #   An existing Azure Managed Identity. aad-pod-identity installed into your cluster. If you are running ASO on an Azure Kubernetes Service (AKS) cluster, you can instead use the integrated aad-pod-identity.  First, set the following environment variables:\nexport IDENTITY_RESOURCE_GROUP=\u0026#34;myrg\u0026#34; # The resource group containing the managed identity. export IDENTITY_NAME=\u0026#34;myidentity\u0026#34; # The name of the identity. export AZURE_SUBSCRIPTION_ID=\u0026#34;00000000-0000-0000-0000-00000000000\u0026#34; # The Azure Subscription ID the identity is in. export AZURE_TENANT_ID=\u0026#34;00000000-0000-0000-0000-00000000000\u0026#34; # The Azure AAD Tenant the identity/subscription is associated with. Use the az cli to get some more details about the identity to use:\nexport IDENTITY_CLIENT_ID=\u0026#34;$(az identity show -g ${IDENTITY_RESOURCE_GROUP} -n ${IDENTITY_NAME} --query clientId -otsv)\u0026#34; export IDENTITY_RESOURCE_ID=\u0026#34;$(az identity show -g ${IDENTITY_RESOURCE_GROUP} -n ${IDENTITY_NAME} --query id -otsv)\u0026#34; Deploy an AzureIdentity:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: \u0026#34;aadpodidentity.k8s.io/v1\u0026#34; kind: AzureIdentity metadata: name: azureschemaoperator-identity namespace: azureschemaoperator-system spec: type: 0 resourceID: ${IDENTITY_RESOURCE_ID} clientID: ${IDENTITY_CLIENT_ID} EOF Deploy an AzureIdentityBinding to bind this identity to the Azure Schema Operator manager pod:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: \u0026#34;aadpodidentity.k8s.io/v1\u0026#34; kind: AzureIdentityBinding metadata: name: azureschemaoperator-identity-binding namespace: azureschemaoperator-system spec: azureIdentity: azureschemaoperator-identity selector: azureschemaoperator-manager-binding EOF Service Principal #  Prerequisites #   An existing Azure Service Principal.  To use Service Principal authentication, specify an schema-operator-controller-settings secret with AZURE_CLIENT_ID and AZURE_CLIENT_SECRET set.\n AZURE_CLIENT_ID must be set to the Service Principal client ID. This will be a GUID. AZURE_CLIENT_SECRET must be set to the Service Principal client secret.  For more information about Service Principals, see creating an Azure Service Principal using the Azure CLI. The AZURE_CLIENT_ID is sometimes also called the App ID. The AZURE_CLIENT_SECRET is the \u0026ldquo;password\u0026rdquo; returned by the command in the previously linked documentation.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: schema-operator-controller-settings namespace: azureschemaoperator-system stringData: AZURE_TENANT_ID: \u0026#34;$AZURE_TENANT_ID\u0026#34; AZURE_CLIENT_ID: \u0026#34;$AZURE_CLIENT_ID\u0026#34; AZURE_CLIENT_SECRET: \u0026#34;$AZURE_CLIENT_SECRET\u0026#34; EOF Note - When deploying the schema operator via the helm chart the secret can be generated by passing createAzureOperatorSecret: true to the Values.yaml.\n"},{"id":5,"href":"/azure-schema-operator/introduction/configuration/","title":"Configuration","section":"User’s Guide","content":"Configuration #  Schema ConfigMap #  SQL Server #  The SQL SERVER configmap supports a few extra options:\n sqlpackageOptions - a string of options (space seperated) to pass to the sqlpackage executable.  "},{"id":6,"href":"/azure-schema-operator/introduction/events_and_conditions/","title":"Events and Conditions","section":"User’s Guide","content":"Events and Conditions #  Azure Schema Operator exposes varios events and conditions to allow for simpler status observation.\nConditions #  We can wait for a SchemaDeployment to finish by waiting on the Execution condition:\n➜ kubectl wait --for=condition=Execution --timeout=10s schemadeployment/master-test-template schemadeployment.dbschema.microsoft.com/master-test-template condition met The condition is also displayed when we get the SchemaDeployment object:\n➜ kubectl get schemadeployments master-test-template NAME TYPE EXECUTED master-test-template kusto True In case of failuer the Execution condition will be marked as such:\n➜ kubectl get schemadeployments master-test-template NAME TYPE EXECUTED master-test-template kusto False Events #  Dureing the deployment process events will be reported on the different steps and changes that occur.\nEvents differ according to the type and required deployment, and can easily be seen from kubectl:\n1 2 3 4  ➜ kubectl get events --field-selector involvedObject.name=master-test-template LAST SEEN TYPE REASON OBJECT MESSAGE 6m36s Normal Created schemadeployment/master-test-template Created versioned deployment \u0026#34;master-test-template-0\u0026#34; 5m36s Normal Executed schemadeployment/master-test-template Scheme was deployed   "},{"id":7,"href":"/azure-schema-operator/introduction/filter/","title":"Filter","section":"User’s Guide","content":"Target Filter #  The operator uses the TargetFilter struct to Filter and aquire execution targets. The semantics change a bit between the different db technologies to match common use patterns.\nAlthough the general notions stay the same.\nClusterUris holds a list of clusters/servers/Eventhub namespaces. Create flag indicates if we should create the DB/schema/registry if missing. Regexp flag indicates if we should regard the filter values as regular expressions or exact match.\nKusto filtering #  In Kusto a common multi-tenantcy solution is DB per tenant, To support this scenario we use the DB field as a regular expression to filter all the database names in the clusters.\nSometimes an external system is used to determain the schema type instead of DB name, e.g. if we have different tier users. To support this scenario we have a Webhook \u0026amp; Label system, we will make a rest call to that webhook and passing the label. The response is expected to be a json array with database names on which we should apply the schema.\nSQL Server filtering #  In Sql Server a common multi-tenantcy solution is Schema per tenant, to support this scenario we use the Schema field as a regular expression to filter all the schema names. Note In this scenario we regard the DB name as exact match.\nEventhub schema registry #  In Eventhubs we can only define one schema registry - we match the name according to the DB field.\n"},{"id":8,"href":"/azure-schema-operator/introduction/install/","title":"Install","section":"User’s Guide","content":"Installation Guide #  This document will guide you through the installation and configuration process. The Schema-Operator needs a client account that can access the Azure Data Explorer clusters you wish to manage and as this is an Operator we need an AKS cluster.\nIdentity #  Schema operator uses a managed identity (MSI) to access kusto resources. Please create a managed identity and assign administrative permissions for the operator to change the databases.\nDeployment in Dev environment #  export ACR=\u0026lt;your acr\u0026gt; export VERSION=0.0.4 export OPERATOR_IMG=\u0026#34;${ACR}.azurecr.io/schema-operator:v${VERSION}\u0026#34; az acr login -n ${ACR} make docker-build-push IMG=$OPERATOR_IMG make deploy IMG=$OPERATOR_IMG Deployment #  Schema-Operator is deployed using a helm chart. In the provided values we need to pass the MSI name to bind.\nexport VERSION=1.0.1 chart=https://github.com/microsoft/azure-schema-operator/releases/download/v${VERSION}/azure-schema-operator-v${VERSION}.tgz chart=charts/azure-schema-operator-v${VERSION}.tgz helm install schema-operator-test $chart --namespace=schema-operator-test --create-namespace more details on chart parameters can be found at the chart docs\n"},{"id":9,"href":"/azure-schema-operator/tutorials/eventhub/","title":"Eventhub","section":"Tutorials","content":"Eventhub schema regitery tutorial #  Creating the ConfigMap:\nkubectl create configmap event-demo --from-literal templateName=\u0026#34;schemaop\u0026#34; --from-literal group=\u0026#34;testsgr\u0026#34; \\ --from-file=schema=./docs/assets/avro-schema.json next we need to define a SchemaDeployment object that will reference the ConfigMap.\napiVersion: dbschema.microsoft.com/v1alpha1 kind: SchemaDeployment metadata: name: eventhub-schema-demo spec: type: eventhub applyTo: clusterUris: [\u0026#39;schematest.servicebus.windows.net\u0026#39;] failIfDataLoss: false failurePolicy: abort source: name: event-demo namespace: default and apply it via kubectl:\nkubectl apply -f ./docs/assets/eventhub-schema-demo.yaml To demonstrate schema evolution, we will add a new field:\n{ \u0026#34;name\u0026#34;: \u0026#34;description\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } let\u0026rsquo;s run:\nkubectl create configmap event-demo --from-literal templateName=\u0026#34;schemaop\u0026#34; --from-literal group=\u0026#34;testsgr\u0026#34; \\ --from-file=schema=./docs/assets/avro-schema-v2.json --dry-run=client -o yaml | kubectl apply -f - To see the deplopyment status we can check the history:\nkubectl schemaop history --namespace default --name eventhub-schema-demo NAMESPACE NAME REVISION default eventhub-schema-demo-0 0 default eventhub-schema-demo-1 1 "},{"id":10,"href":"/azure-schema-operator/tutorials/kusto/","title":"Kusto","section":"Tutorials","content":"Azure Data Explorer (ADX, AKA Kusto) Tutorial #  In this short tutorial we will review the process of managing a schema, performing a change and rollback in the case of an error.\nPre-requisits #  the tutorial assumes that the Schema operator is already installed with the appropriate permissions - if not, please see installation\nTutorial steps #  We start by creating a KQL file following the delta-kusto instructions\nOr simply download a sample kql file\nOnce we have a kql file we need to generate a ConfigMap definition file:\nkubectl create configmap test-sample-kql --from-file=kql=/Users/jocohe/Documents/delta-kusto/sample.kql --dry-run=client -o json | jq . We reference the ConfigMap from our SchemaDeployment object (name to be changed\u0026hellip;) to apply to our clusters\nkubectl apply -f /Users/jocohe/Documents/delta-kusto/template-demo.yml To update the schema to a new version we can simply apply a new ConfigMap:\nkubectl apply -f /Users/jocohe/Documents/delta-kusto/cm-dev2.yml or use the kuebctl schemaop plugin:\nkubectl schemaop update --namespace default --name dev-test-kql --schema-file /Users/jocohe/Documents/delta-kusto/dev-state.kql To view current status and history we can use the provided plugin:\nkubectl schemaop status --namespace default --name master-test-template NAMESPACE NAME REVISION EXECUTED FAILED RUNNING SUCCEEDED default master-test-template-1 1 true 0 0 1 kubectl schemaop history --namespace default --name master-test-template NAMESPACE NAME REVISION default master-test-template-0 0 default master-test-template-1 1 Now, lets make things a bit more interesting, and apply a bad schema:\nkubectl apply -f /Users/jocohe/Documents/delta-kusto/cm-dev-err.yml If we check the status we will see we are now on our 4th revision!\nkubectl schemaop history --namespace default --name master-test-template NAMESPACE NAME REVISION default master-test-template-0 0 default master-test-template-1 1 default master-test-template-2 2 default master-test-template-3 3 further checking the history we can see that revisio 2 (the 3rd revision) failed:\nkubectl schemaop history --namespace default --name master-test-template --revision 2 NAMESPACE NAME REVISION EXECUTED FAILED RUNNING SUCCEEDED default master-test-template-2 2 false 1 1 0 and we can see that the 4th and 2nd revisions are the same - we rolled back the bad revision as requested by the template object:\nspec: failurePolicy: rollback Summary #  We\u0026rsquo;ve looked at a common flow and how Schema-Operator can ease managment of large schema deployments. We suggest trying some of these scenarios in a dev environment to get familiar with the tool.\n"},{"id":11,"href":"/azure-schema-operator/tutorials/sqlserver/","title":"Sqlserver","section":"Tutorials","content":"SQL Server Tutorial #  A tutorial for a simple scenario of a single db in a server with schema per tenant.\nwe assume MSI is used to authenticate (because it\u0026rsquo;s simpler :) )\nwe need to add the MSI as a user to the DB:\nCREATE USER [dbset-operator-dataops-msi-erx-qds] FROM EXTERNAL PROVIDER; GO ALTER ROLE db_datareader ADD MEMBER [dbset-operator-dataops-msi-erx-qds]; ALTER ROLE db_datawriter ADD MEMBER [dbset-operator-dataops-msi-erx-qds]; ALTER ROLE db_owner ADD MEMBER [dbset-operator-dataops-msi-erx-qds]; GRANT EXECUTE TO [dbset-operator-dataops-msi-erx-qds] GO Creating the ConfigMap:\nkubectl create configmap dacpac-config --from-literal templateName=\u0026#34;SalesLT\u0026#34; --from-file=dacpac=./docs/assets/test.dacpac next we need to define a SchemaDeployment object that will reference the ConfigMap.\napiVersion: dbschema.microsoft.com/v1alpha1 kind: SchemaDeployment metadata: name: sql-demo-deployment spec: type: sqlServer applyTo: clusterUris: [\u0026#39;schematest.database.windows.net\u0026#39;] db: \u0026#39;db1\u0026#39; schema: test failIfDataLoss: true failurePolicy: abort source: name: dacpac-config namespace: default and apply it via kubectl:\nkubectl apply -f ./docs/assets/sql-demo-deployment.yaml External Dacpacs #  In case our project has external dacpac references we can add them as a reference from the schema ConfigMap:\nkubectl create configmap common-config --from-file=dacpac=./DBOCommon.dacpac kubectl create configmap tenant-config --from-literal templateName=\u0026#34;MasterSchema\u0026#34; \\ --from-literal externalDacpacs=\u0026#39;{ \u0026#34;DBOCommon\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;common-config\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;}}\u0026#39; \\ --from-file=dacpac=./SevilleSqlDbTenant.dacpac The external Dacpac requires a seperate external SchemaDeployment object to deploy it ( to fully capsulate the \u0026ldquo;externallism\u0026rdquo; of it)\nNote as the name of the external DacPac matters we need to pass this name - so it is the \u0026ldquo;key\u0026rdquo; for the reference.\n"}]